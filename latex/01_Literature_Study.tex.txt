\documentclass[../report.tex]{subfiles}
\begin{document}
\chapter{Literature Study}
\label{ch:literature_study}

A thesis project always commences with a study of pre-existing literature, this is intended to introduce the main concepts that will be used as the foundation of the methodologies used, as well as supplement the discussion of any results gathered. To conduct a literature study is to survey the vast wealth of material that has been produced by researchers and practitioners of the fields related to a research project, thus the first field to be surveyed will be that of Reinforcement Learning, and the second field to be surveyed is the developments of the Flying-V. 


\section{Reinforcement Learning Foundations}

The basic idea of reinforcement learning is to have some learner associate rewards with actions that helps realise a goal, and promote the learner to take more of such actions by asking it to maximize the reward. The learner is not told what the goal is explicitly, its' only interface with the world around it is through executing these actions, and observing that it has transitioned into some state and received some reward. Reinforcement learning can be thought of as a way in which theorists have attempted to codify, and formulate algorithms for, the universal experience of learning through trial and error. Much like how a child learns to walk, or a dog learns to sit, it is through reinforcement learning that machines can learn how to perform tasks that might not be easily programmed. Through this codification, computer programmes have been made which demonstrated impressive levels of learning; for example programmes can learn through reinforcement learning to play various high-dimensional board games to a level of expertise surpassing any living player \cite{alpha_zero}, and a robotic arm can learn hand dexterity and mimick the hand movements of a human \cite{OpenAI_dexterity}.

Understanding how the reinforcement learning algorithms work behind such examples and how they may be applied to flight control will require understanding it's foundations first, thus this section will introduce the basic terminologies and ideas used to build these algorithms.


\subsection{Markov Decision Process}

The Markov decision process is the mathematical framework that are used to model sequential decision processes such as how an agent interacts with an environment, and it is what contextualizes all the ideas in reinforcement learning, for instance the basic notion that an agent performs an action and receives a reward.

In such a framework, there exists only two entities: the agent and the environment, and information flows from one entity to another to model making decisions and their consequences. In reinforcement learning the agent is sometimes also called the learner, it selects an action $A_t$ which gets fed to the environment, and the environment will provide the corresponding state $S_{t+1}$ which the agent has transitioned to as a result of action $A_t$ and the reward associated to that state transition $R_{t+1}$, the agent can subsequently use $S_{t+1}$ and $R_{t+1}$ to decide on the next time step's action $A_{t+1}$. A graphical depiction of this agent-environment interface in the Markov decision process is shown in Figure ....

\begin{figure}

\end{figure}




This time evolution of an agent interacting with the enironment and the environment responding accordingly is recorded in a so-called trajectory, which is a chain of state-action-reward for the entire duration of the decision process in the form expressed in \autoref{eq:trajectory}.

\begin{equation} \label{eq:trajectory}
    S_{0}, A_{0}, R_{1}, S_{1}, A_{1}, R_{2}, \dots, S_{T-1}, A_{T-1}, R_{T}, S_{T}, A_{T}
\end{equation}



 Figure xxx shows this idea in a graphical form.

Markov decision processes are used to model sequential decision processes, and  a simple illustrative example is shown in Figure xxx for the case of a warehouse robot. 

\textbf{Distinction between Reward and Return}

In reinforcement learning nomenclature, a distinction in terminology exists between the reward being received every time step, and the cumulative reward that an agent receives over many time steps. While rewards are received every time step, when they are summed up over time, they are then referred to as the \textit{return}. For example, an episodic return refers to how much cumulative reward an agent has received over the span of an episode. 

\subsection{Value Function}

Value functions describes how much reward in total an agent will receive in the expectation. Two types of value functions are commonly used in reinforcement learning, the state-value function and the action-value function. The state-value function is defined as a mapping from any state to the expected return from that state onwards, mathematically this is defined as:

\begin{equation}
    V_{\pi}(s) = \mathbb{E}_{\pi}[G_t | S_t = s]
\end{equation}

Where $V_{\pi}(s)$ is the value of state $s$, and $G_t$ is the return from time $t$ onwards when following the policy $\pi$. The state-value function can be intuitively understood as how worthwhile it is to be in a certain state. The action-value function is defined as a mapping from any state-action pair to the expected return from that state-action pair onwards, this can again mathematically stated as follows:

\begin{equation}
    Q_{\pi}(s, a) = \mathbb{E}_{\pi}[G_t | S_t = s, A_t = a]
\end{equation}

Where $Q_{\pi}(s, a)$ is the value of taking action $a$ in state $s$, and $G_t$ is the return from time $t$ onwards when following the policy $\pi$. The action-value function can be intuitively understood as how worthwhile it is to take a certain action in a certain state.

\subsection{Bellman Equations}

\subsection{Policy}

\subsection{Environment}


\section{Dynamic Programming}

\subsection{Adaptive Critic Design}


\section{Deep Reinforcement Learning}

\subsection{Temporal Difference Learning}

\subsection{Deep Learning}

Deep learning refers to the subfield of machine learning which studies deep neural networks and their applications. This field has its origins in the ideas of artificial neural networks and is a scientific effort that has dramatically changed the idea of what an artificial neural network is and what it might be capable of. Deep neural networks fundamentally are an expansion of artificial neural networks, they use the same basic architecture of a neural network, with layers of interconnected neurons between a surface or input layer and a final output layer. The distinction between deep and artificial neural networks is that deep neural networks use many more layers and nodes than are typical in artificial networks,

\subsection{Merging TD \& Deep Learning}

Basic temporal difference algorithms utilize tabular look ups to retreive the value function of each state or state-action pair, this can hinder scaling to larger problem spaces which have many state or action variables, as the number of values to be stored increases combinatorially. The idea was thus put forth that function approximators should be used to approximate these look up tables which provided the scaling power necessary for higher dimensional tasks \cite{function_approximators}, with deep neural networks and multi layer perceptrons becoming popular approximators as they demonstrate a better scalability and approximation power \cite{drl_benchmarking}\cite{drl_atari}\cite{drl_humanlvl}. However, these function approximators need not necessarily be deep neural networks; in fact, early works focused on the application of simpler and smaller-scale approximations. Such as using coarse tile encoding, linear polynomials, Fourier basis, and many more.


\section{Synopsis}



\end{document}